#!/usr/bin/env python3
"""
ClusterFusion Attention Kernel Test for Pythia-2.8B

This test validates the CUDA-accelerated Attention + MLP Up + GELU kernel
and benchmarks its performance against the PyTorch baseline.
"""

import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
import torch.nn.functional as F
import time
from transformers import AutoModelForCausalLM, AutoTokenizer

# Model configuration
MODEL_NAME = "EleutherAI/pythia-2.8b"
HIDDEN_DIM = 2560
NUM_HEADS = 32
HEAD_DIM = 80
ROTARY_DIM = 20
FFN_DIM = 10240
NUM_LAYERS = 32

def precompute_rope(max_pos, device="cuda:0"):
    """Precompute rotary position embeddings."""
    inv_freq = 1.0 / (10000 ** (torch.arange(0, ROTARY_DIM, 2, device=device).float() / ROTARY_DIM))
    positions = torch.arange(max_pos, device=device, dtype=torch.float32)
    freqs = torch.outer(positions, inv_freq)
    emb = torch.cat([freqs, freqs], dim=-1)
    cos = emb.cos()
    sin = emb.sin()
    cos = torch.cat([cos, torch.ones((max_pos, HEAD_DIM - ROTARY_DIM), device=device)], -1)
    sin = torch.cat([sin, torch.zeros((max_pos, HEAD_DIM - ROTARY_DIM), device=device)], -1)
    return cos, sin


def pytorch_attention_mlp_up(hidden, layer, k_cache, v_cache, cos, sin, seq_len):
    """PyTorch baseline for Attention + MLP Up + GELU."""
    # LayerNorm
    ln_out = F.layer_norm(hidden, (HIDDEN_DIM,), 
                          layer.input_layernorm.weight, 
                          layer.input_layernorm.bias)
    
    # QKV Projection
    qkv = F.linear(ln_out, layer.attention.query_key_value.weight, 
                   layer.attention.query_key_value.bias)
    q, k, v = qkv.chunk(3, dim=-1)
    
    # Reshape for multi-head attention
    q = q.view(1, 1, NUM_HEADS, HEAD_DIM)
    k = k.view(1, 1, NUM_HEADS, HEAD_DIM)
    v = v.view(1, 1, NUM_HEADS, HEAD_DIM)
    
    # RoPE
    cos_pos = cos[seq_len].view(1, 1, 1, HEAD_DIM)
    sin_pos = sin[seq_len].view(1, 1, 1, HEAD_DIM)
    
    def apply_rope(x):
        x_rot = x[..., :ROTARY_DIM]
        x_pass = x[..., ROTARY_DIM:]
        x1 = x_rot[..., :ROTARY_DIM//2]
        x2 = x_rot[..., ROTARY_DIM//2:]
        rotated = torch.cat([-x2, x1], dim=-1)
        x_rotated = x_rot * cos_pos[..., :ROTARY_DIM] + rotated * sin_pos[..., :ROTARY_DIM]
        return torch.cat([x_rotated, x_pass], dim=-1)
    
    q = apply_rope(q)
    k = apply_rope(k)
    
    # Update KV cache
    k_flat = k.view(1, HIDDEN_DIM)
    v_flat = v.view(1, HIDDEN_DIM)
    k_cache[seq_len] = k_flat
    v_cache[seq_len] = v_flat
    
    # Attention
    k_cached = k_cache[:seq_len + 1].view(seq_len + 1, NUM_HEADS, HEAD_DIM)
    v_cached = v_cache[:seq_len + 1].view(seq_len + 1, NUM_HEADS, HEAD_DIM)
    
    q = q.squeeze(0).squeeze(0)  # [NUM_HEADS, HEAD_DIM]
    
    attn_scores = torch.einsum('hd,shd->hs', q, k_cached) / (HEAD_DIM ** 0.5)
    attn_probs = F.softmax(attn_scores, dim=-1)
    attn_out = torch.einsum('hs,shd->hd', attn_probs, v_cached)
    attn_out = attn_out.view(1, HIDDEN_DIM)
    
    # Output projection
    attn_output = F.linear(attn_out, layer.attention.dense.weight, 
                           layer.attention.dense.bias)
    
    # Post-attention LayerNorm
    post_ln_out = F.layer_norm(hidden.squeeze(0), (HIDDEN_DIM,),
                               layer.post_attention_layernorm.weight,
                               layer.post_attention_layernorm.bias)
    
    # MLP Up + GELU
    mlp_intermediate = F.linear(post_ln_out, layer.mlp.dense_h_to_4h.weight,
                                layer.mlp.dense_h_to_4h.bias)
    mlp_intermediate = F.gelu(mlp_intermediate)
    
    return attn_output, mlp_intermediate


def test_correctness(model, device):
    """Test kernel correctness against PyTorch baseline."""
    print("\n" + "=" * 70)
    print("Correctness Test")
    print("=" * 70)
    
    import clusterfusion
    
    layer = model.gpt_neox.layers[0]
    seq_len = 64
    max_seq_len = seq_len + 100
    all_cos, all_sin = precompute_rope(max_seq_len, device)
    cos, sin = all_cos[seq_len], all_sin[seq_len]
    
    # Input
    input_hidden = torch.randn(1, 1, HIDDEN_DIM, device=device, dtype=torch.float16)
    
    # Prepare weights
    qkv_weight = layer.attention.query_key_value.weight.T.contiguous()
    qkv_bias = layer.attention.query_key_value.bias.contiguous()
    o_weight = layer.attention.dense.weight.T.contiguous()
    o_bias = layer.attention.dense.bias.contiguous()
    ln_weight = layer.input_layernorm.weight.contiguous()
    ln_bias = layer.input_layernorm.bias.contiguous()
    post_ln_weight = layer.post_attention_layernorm.weight.contiguous()
    post_ln_bias = layer.post_attention_layernorm.bias.contiguous()
    mlp_up_weight = layer.mlp.dense_h_to_4h.weight.T.contiguous()
    mlp_up_bias = layer.mlp.dense_h_to_4h.bias.contiguous()
    
    # CUDA kernel
    k_cache_cuda = torch.zeros(max_seq_len, HIDDEN_DIM, device=device, dtype=torch.float16)
    v_cache_cuda = torch.zeros(max_seq_len, HIDDEN_DIM, device=device, dtype=torch.float16)
    
    attn_out_cuda, mlp_int_cuda, _, _ = clusterfusion.pythia_2b8_attention_only(
        input_hidden,
        qkv_weight, qkv_bias,
        o_weight, o_bias,
        k_cache_cuda, v_cache_cuda,
        ln_weight, ln_bias,
        cos, sin,
        post_ln_weight, post_ln_bias,
        mlp_up_weight, mlp_up_bias,
        seq_len
    )
    
    # PyTorch baseline
    k_cache_pt = torch.zeros(max_seq_len, HIDDEN_DIM, device=device, dtype=torch.float16)
    v_cache_pt = torch.zeros(max_seq_len, HIDDEN_DIM, device=device, dtype=torch.float16)
    
    attn_out_pt, mlp_int_pt = pytorch_attention_mlp_up(
        input_hidden, layer, k_cache_pt, v_cache_pt, all_cos, all_sin, seq_len
    )
    
    # Compare
    attn_diff = (attn_out_cuda - attn_out_pt).abs().max().item()
    mlp_diff = (mlp_int_cuda - mlp_int_pt).abs().max().item()
    
    print(f"\nAttention output max diff: {attn_diff:.6f}")
    print(f"MLP intermediate max diff: {mlp_diff:.6f}")
    
    if attn_diff < 0.1 and mlp_diff < 0.1:
        print("\n✅ Correctness test PASSED!")
    else:
        print("\n⚠️  Some differences detected (expected for FP16)")
    
    return True


def benchmark(model, device):
    """Benchmark CUDA kernel vs PyTorch baseline."""
    print("\n" + "=" * 70)
    print("Performance Benchmark")
    print("=" * 70)
    
    import clusterfusion
    
    layer = model.gpt_neox.layers[0]
    seq_len = 64
    max_seq_len = seq_len + 100
    all_cos, all_sin = precompute_rope(max_seq_len, device)
    cos, sin = all_cos[seq_len], all_sin[seq_len]
    
    input_hidden = torch.randn(1, 1, HIDDEN_DIM, device=device, dtype=torch.float16)
    
    # Prepare weights
    qkv_weight = layer.attention.query_key_value.weight.T.contiguous()
    qkv_bias = layer.attention.query_key_value.bias.contiguous()
    o_weight = layer.attention.dense.weight.T.contiguous()
    o_bias = layer.attention.dense.bias.contiguous()
    ln_weight = layer.input_layernorm.weight.contiguous()
    ln_bias = layer.input_layernorm.bias.contiguous()
    post_ln_weight = layer.post_attention_layernorm.weight.contiguous()
    post_ln_bias = layer.post_attention_layernorm.bias.contiguous()
    mlp_up_weight = layer.mlp.dense_h_to_4h.weight.T.contiguous()
    mlp_up_bias = layer.mlp.dense_h_to_4h.bias.contiguous()
    mlp_down_weight = layer.mlp.dense_4h_to_h.weight.half()
    mlp_down_bias = layer.mlp.dense_4h_to_h.bias.half()
    
    WARMUP = 50
    ITERS = 200
    
    # ========== CUDA Benchmark ==========
    k_cache = torch.zeros(max_seq_len, HIDDEN_DIM, device=device, dtype=torch.float16)
    v_cache = torch.zeros(max_seq_len, HIDDEN_DIM, device=device, dtype=torch.float16)
    
    for _ in range(WARMUP):
        attn, mlp_int, _, _ = clusterfusion.pythia_2b8_attention_only(
            input_hidden, qkv_weight, qkv_bias, o_weight, o_bias,
            k_cache, v_cache, ln_weight, ln_bias, cos, sin,
            post_ln_weight, post_ln_bias, mlp_up_weight, mlp_up_bias, seq_len
        )
        mlp_down = F.linear(mlp_int.unsqueeze(0), mlp_down_weight, mlp_down_bias)
        output = input_hidden.squeeze(0) + attn + mlp_down
    torch.cuda.synchronize()
    
    start = time.perf_counter()
    for _ in range(ITERS):
        attn, mlp_int, _, _ = clusterfusion.pythia_2b8_attention_only(
            input_hidden, qkv_weight, qkv_bias, o_weight, o_bias,
            k_cache, v_cache, ln_weight, ln_bias, cos, sin,
            post_ln_weight, post_ln_bias, mlp_up_weight, mlp_up_bias, seq_len
        )
        mlp_down = F.linear(mlp_int.unsqueeze(0), mlp_down_weight, mlp_down_bias)
        output = input_hidden.squeeze(0) + attn + mlp_down
    torch.cuda.synchronize()
    cuda_time = (time.perf_counter() - start) / ITERS * 1000
    
    # ========== PyTorch Benchmark ==========
    k_cache_pt = torch.zeros(max_seq_len, HIDDEN_DIM, device=device, dtype=torch.float16)
    v_cache_pt = torch.zeros(max_seq_len, HIDDEN_DIM, device=device, dtype=torch.float16)
    
    for _ in range(WARMUP):
        attn_pt, mlp_int_pt = pytorch_attention_mlp_up(
            input_hidden, layer, k_cache_pt, v_cache_pt, all_cos, all_sin, seq_len
        )
        mlp_down = F.linear(mlp_int_pt, mlp_down_weight, mlp_down_bias)
        output = input_hidden.squeeze(0) + attn_pt + mlp_down
    torch.cuda.synchronize()
    
    start = time.perf_counter()
    for _ in range(ITERS):
        attn_pt, mlp_int_pt = pytorch_attention_mlp_up(
            input_hidden, layer, k_cache_pt, v_cache_pt, all_cos, all_sin, seq_len
        )
        mlp_down = F.linear(mlp_int_pt, mlp_down_weight, mlp_down_bias)
        output = input_hidden.squeeze(0) + attn_pt + mlp_down
    torch.cuda.synchronize()
    pytorch_time = (time.perf_counter() - start) / ITERS * 1000
    
    # ========== Results ==========
    speedup = pytorch_time / cuda_time
    
    print(f"\nPer-layer timing (batch=1, seq_len={seq_len}):")
    print(f"  PyTorch:       {pytorch_time:.4f} ms")
    print(f"  ClusterFusion: {cuda_time:.4f} ms")
    print(f"  Speedup:       {speedup:.2f}x")
    
    print(f"\n32-layer projection:")
    print(f"  PyTorch:       {pytorch_time * NUM_LAYERS:.2f} ms")
    print(f"  ClusterFusion: {cuda_time * NUM_LAYERS:.2f} ms")
    
    print("\n" + "=" * 70)
    print(f"✅ ClusterFusion achieves {speedup:.2f}x speedup!")
    print("=" * 70)


def main():
    print("=" * 70)
    print("ClusterFusion Attention Kernel for Pythia-2.8B")
    print("=" * 70)
    
    print("\nLoading model...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME, dtype=torch.float16, device_map="cuda:0"
    )
    model.eval()
    device = torch.device("cuda:0")
    
    print(f"Model: {MODEL_NAME}")
    print(f"Hidden: {HIDDEN_DIM}, Heads: {NUM_HEADS}, HeadDim: {HEAD_DIM}")
    print(f"FFN: {FFN_DIM}, Layers: {NUM_LAYERS}")
    
    test_correctness(model, device)
    benchmark(model, device)


if __name__ == "__main__":
    main()
