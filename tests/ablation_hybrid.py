#!/usr/bin/env python3
"""
Ablation Study: Hybrid Branch Speedup

Directly measures speedup by replacing one branch at a time:
1. Full PyTorch (baseline)
2. CUDA Attention+MLPUp + PyTorch MLPDown  
3. PyTorch Attention+MLPUp + CUDA MLPDown
4. Full CUDA (both branches)

This requires separate Python bindings for each branch, which we don't have.
So we use the split kernel and PyTorch implementations to measure each branch separately.
"""

import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
import torch.nn.functional as F
import time
from transformers import AutoModelForCausalLM, AutoTokenizer

# Model configuration
MODEL_NAME = "EleutherAI/pythia-2.8b"
HIDDEN_DIM = 2560
NUM_HEADS = 32
HEAD_DIM = 80
ROTARY_DIM = 20
NUM_LAYERS = 32
FFN_DIM = 10240

# Test configuration
WARMUP_ITERS = 50
BENCHMARK_ITERS = 200
SEQ_LEN = 64

def print_header():
    print("=" * 90)
    print("Ablation Study: Hybrid Branch Speedup")
    print("=" * 90)
    print(f"Model: {MODEL_NAME}")
    print(f"Config: Hidden={HIDDEN_DIM}, Heads={NUM_HEADS}, FFN={FFN_DIM}")
    print(f"Test: {WARMUP_ITERS} warmup + {BENCHMARK_ITERS} iters, seq_len={SEQ_LEN}")
    print("=" * 90)

def load_model():
    print("\nLoading model...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME, dtype=torch.float16, device_map="cuda:0"
    )
    model.eval()
    print(f"Model loaded: {MODEL_NAME}")
    return model, tokenizer

def gelu_approx(x):
    return 0.5 * x * (1 + torch.tanh(0.7978845608 * (x + 0.044715 * x * x * x)))

def pytorch_attention_mlp_up(h, layer, k_cache, v_cache, cos, sin, seq_len):
    """PyTorch: LayerNorm â†’ QKV â†’ RoPE â†’ Attention â†’ Output â†’ PostLN â†’ MLP Up â†’ GELU"""
    # LayerNorm
    ln_out = F.layer_norm(h, (HIDDEN_DIM,), 
                          layer.input_layernorm.weight.half(),
                          layer.input_layernorm.bias.half())
    
    # QKV
    qkv = F.linear(ln_out, layer.attention.query_key_value.weight.half(),
                   layer.attention.query_key_value.bias.half())
    qkv = qkv.view(1, 1, 3, NUM_HEADS, HEAD_DIM)
    q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]
    q = q.transpose(1, 2)  # [1, heads, 1, head_dim]
    k = k.transpose(1, 2)
    v = v.transpose(1, 2)
    
    # RoPE (simplified)
    cos_rot = cos[:ROTARY_DIM].view(1, 1, 1, ROTARY_DIM)
    sin_rot = sin[:ROTARY_DIM].view(1, 1, 1, ROTARY_DIM)
    
    q_rot, q_pass = q[..., :ROTARY_DIM], q[..., ROTARY_DIM:]
    k_rot, k_pass = k[..., :ROTARY_DIM], k[..., ROTARY_DIM:]
    
    q1, q2 = q_rot[..., ::2], q_rot[..., 1::2]
    k1, k2 = k_rot[..., ::2], k_rot[..., 1::2]
    cos_h = cos_rot[..., ::2]
    sin_h = sin_rot[..., ::2]
    
    q_rot_new = torch.cat([q1*cos_h - q2*sin_h, q1*sin_h + q2*cos_h], -1)
    k_rot_new = torch.cat([k1*cos_h - k2*sin_h, k1*sin_h + k2*cos_h], -1)
    
    q = torch.cat([q_rot_new, q_pass], -1)
    k_new = torch.cat([k_rot_new, k_pass], -1)
    
    # Store KV
    k_for_cache = k_new.squeeze(2).reshape(-1)
    v_for_cache = v.squeeze(2).reshape(-1)
    k_cache[seq_len] = k_for_cache
    v_cache[seq_len] = v_for_cache
    
    # Attention
    k_cached = k_cache[:seq_len+1].view(seq_len+1, NUM_HEADS, HEAD_DIM).transpose(0, 1).unsqueeze(0)
    v_cached = v_cache[:seq_len+1].view(seq_len+1, NUM_HEADS, HEAD_DIM).transpose(0, 1).unsqueeze(0)
    
    scale = 1.0 / (HEAD_DIM ** 0.5)
    attn_scores = torch.matmul(q.half(), k_cached.half().transpose(-2, -1)) * scale
    attn_probs = F.softmax(attn_scores.float(), dim=-1).half()
    attn_out = torch.matmul(attn_probs, v_cached.half())
    attn_out = attn_out.transpose(1, 2).reshape(1, 1, HIDDEN_DIM)
    
    # Output projection
    attn_output = F.linear(attn_out, layer.attention.dense.weight.half(),
                           layer.attention.dense.bias.half())
    
    # Post LayerNorm
    post_ln = F.layer_norm(ln_out, (HIDDEN_DIM,),
                           layer.post_attention_layernorm.weight.half(),
                           layer.post_attention_layernorm.bias.half())
    
    # MLP Up + GELU
    mlp_up = F.linear(post_ln, layer.mlp.dense_h_to_4h.weight.half(),
                      layer.mlp.dense_h_to_4h.bias.half())
    mlp_intermediate = gelu_approx(mlp_up)
    
    return mlp_intermediate, attn_output, k_for_cache, v_for_cache

def pytorch_mlp_down(mlp_intermediate, h, attn_output, layer):
    """PyTorch: MLP Down â†’ Residual"""
    mlp_out = F.linear(mlp_intermediate, layer.mlp.dense_4h_to_h.weight.half(),
                       layer.mlp.dense_4h_to_h.bias.half())
    return h + attn_output + mlp_out

def precompute_rope(max_pos, device="cuda:0"):
    inv_freq = 1.0 / (10000 ** (torch.arange(0, ROTARY_DIM, 2, device=device).float() / ROTARY_DIM))
    positions = torch.arange(max_pos, device=device, dtype=torch.float32)
    freqs = torch.outer(positions, inv_freq)
    emb = torch.cat([freqs, freqs], dim=-1)
    cos = emb.cos()
    sin = emb.sin()
    padding = max_pos
    cos = torch.cat([cos, torch.ones((max_pos, HEAD_DIM - ROTARY_DIM), device=device)], -1)
    sin = torch.cat([sin, torch.zeros((max_pos, HEAD_DIM - ROTARY_DIM), device=device)], -1)
    return cos, sin

def benchmark_full_pytorch(model, layers):
    """Full PyTorch baseline."""
    device = torch.device("cuda:0")
    max_seq = SEQ_LEN + 100
    all_cos, all_sin = precompute_rope(max_seq, device)
    k_caches = [torch.zeros(max_seq, HIDDEN_DIM, device=device, dtype=torch.float16) for _ in range(NUM_LAYERS)]
    v_caches = [torch.zeros(max_seq, HIDDEN_DIM, device=device, dtype=torch.float16) for _ in range(NUM_LAYERS)]
    
    h = torch.randn(1, 1, HIDDEN_DIM, device=device, dtype=torch.float16)
    cos, sin = all_cos[SEQ_LEN], all_sin[SEQ_LEN]
    
    # Warmup
    for _ in range(WARMUP_ITERS):
        h_t = h.clone()
        for i, layer in enumerate(layers):
            mlp_inter, attn_out, _, _ = pytorch_attention_mlp_up(h_t, layer, k_caches[i], v_caches[i], cos, sin, SEQ_LEN)
            h_t = pytorch_mlp_down(mlp_inter, h_t, attn_out, layer)
    torch.cuda.synchronize()
    
    # Benchmark
    start = time.perf_counter()
    for _ in range(BENCHMARK_ITERS):
        h_t = h.clone()
        for i, layer in enumerate(layers):
            mlp_inter, attn_out, _, _ = pytorch_attention_mlp_up(h_t, layer, k_caches[i], v_caches[i], cos, sin, SEQ_LEN)
            h_t = pytorch_mlp_down(mlp_inter, h_t, attn_out, layer)
    torch.cuda.synchronize()
    end = time.perf_counter()
    
    return (end - start) / BENCHMARK_ITERS * 1000

def benchmark_pytorch_attn_only(model, layers):
    """Benchmark PyTorch Attention+MLPUp only."""
    device = torch.device("cuda:0")
    max_seq = SEQ_LEN + 100
    all_cos, all_sin = precompute_rope(max_seq, device)
    k_caches = [torch.zeros(max_seq, HIDDEN_DIM, device=device, dtype=torch.float16) for _ in range(NUM_LAYERS)]
    v_caches = [torch.zeros(max_seq, HIDDEN_DIM, device=device, dtype=torch.float16) for _ in range(NUM_LAYERS)]
    
    h = torch.randn(1, 1, HIDDEN_DIM, device=device, dtype=torch.float16)
    cos, sin = all_cos[SEQ_LEN], all_sin[SEQ_LEN]
    
    # Warmup
    for _ in range(WARMUP_ITERS):
        h_t = h.clone()
        for i, layer in enumerate(layers):
            _, _, _, _ = pytorch_attention_mlp_up(h_t, layer, k_caches[i], v_caches[i], cos, sin, SEQ_LEN)
    torch.cuda.synchronize()
    
    # Benchmark
    start = time.perf_counter()
    for _ in range(BENCHMARK_ITERS):
        h_t = h.clone()
        for i, layer in enumerate(layers):
            _, _, _, _ = pytorch_attention_mlp_up(h_t, layer, k_caches[i], v_caches[i], cos, sin, SEQ_LEN)
    torch.cuda.synchronize()
    end = time.perf_counter()
    
    return (end - start) / BENCHMARK_ITERS * 1000

def benchmark_pytorch_mlp_only(model, layers):
    """Benchmark PyTorch MLPDown only."""
    device = torch.device("cuda:0")
    
    h = torch.randn(1, 1, HIDDEN_DIM, device=device, dtype=torch.float16)
    mlp_inter = torch.randn(1, 1, FFN_DIM, device=device, dtype=torch.float16)
    attn_out = torch.randn(1, 1, HIDDEN_DIM, device=device, dtype=torch.float16)
    
    # Warmup
    for _ in range(WARMUP_ITERS):
        h_t = h.clone()
        for layer in layers:
            h_t = pytorch_mlp_down(mlp_inter, h_t, attn_out, layer)
    torch.cuda.synchronize()
    
    # Benchmark
    start = time.perf_counter()
    for _ in range(BENCHMARK_ITERS):
        h_t = h.clone()
        for layer in layers:
            h_t = pytorch_mlp_down(mlp_inter, h_t, attn_out, layer)
    torch.cuda.synchronize()
    end = time.perf_counter()
    
    return (end - start) / BENCHMARK_ITERS * 1000

def benchmark_full_cuda(model):
    """Full CUDA fused kernel."""
    import clusterfusion
    
    device = torch.device("cuda:0")
    max_seq = SEQ_LEN + 100
    all_cos, all_sin = precompute_rope(max_seq, device)
    k_caches = [torch.zeros(max_seq, HIDDEN_DIM, device=device, dtype=torch.float16) for _ in range(NUM_LAYERS)]
    v_caches = [torch.zeros(max_seq, HIDDEN_DIM, device=device, dtype=torch.float16) for _ in range(NUM_LAYERS)]
    
    # Pre-extract weights
    all_weights = []
    for layer in model.gpt_neox.layers:
        all_weights.append({
            "qkv_weight": layer.attention.query_key_value.weight.T.contiguous(),
            "qkv_bias": layer.attention.query_key_value.bias.contiguous(),
            "o_weight": layer.attention.dense.weight.T.contiguous(),
            "o_bias": layer.attention.dense.bias.contiguous(),
            "ln_weight": layer.input_layernorm.weight.contiguous(),
            "ln_bias": layer.input_layernorm.bias.contiguous(),
            "post_ln_weight": layer.post_attention_layernorm.weight.contiguous(),
            "post_ln_bias": layer.post_attention_layernorm.bias.contiguous(),
            "mlp_up_weight": layer.mlp.dense_h_to_4h.weight.T.contiguous(),
            "mlp_up_bias": layer.mlp.dense_h_to_4h.bias.contiguous(),
            "mlp_down_weight": layer.mlp.dense_4h_to_h.weight.T.contiguous(),
            "mlp_down_bias": layer.mlp.dense_4h_to_h.bias.contiguous(),
        })
    
    h = torch.randn(1, 1, HIDDEN_DIM, device=device, dtype=torch.float16)
    cos, sin = all_cos[SEQ_LEN], all_sin[SEQ_LEN]
    
    # Warmup
    for _ in range(WARMUP_ITERS):
        h_t = h.clone()
        for i in range(NUM_LAYERS):
            w = all_weights[i]
            h_t, _, _ = clusterfusion.pythia_2b8_decoder_layer(
                h_t, w["qkv_weight"], w["qkv_bias"], w["o_weight"], w["o_bias"],
                k_caches[i], v_caches[i], w["ln_weight"], w["ln_bias"], cos, sin,
                w["post_ln_weight"], w["post_ln_bias"],
                w["mlp_up_weight"], w["mlp_up_bias"], w["mlp_down_weight"], w["mlp_down_bias"],
                SEQ_LEN
            )
    torch.cuda.synchronize()
    
    # Benchmark
    start = time.perf_counter()
    for _ in range(BENCHMARK_ITERS):
        h_t = h.clone()
        for i in range(NUM_LAYERS):
            w = all_weights[i]
            h_t, _, _ = clusterfusion.pythia_2b8_decoder_layer(
                h_t, w["qkv_weight"], w["qkv_bias"], w["o_weight"], w["o_bias"],
                k_caches[i], v_caches[i], w["ln_weight"], w["ln_bias"], cos, sin,
                w["post_ln_weight"], w["post_ln_bias"],
                w["mlp_up_weight"], w["mlp_up_bias"], w["mlp_down_weight"], w["mlp_down_bias"],
                SEQ_LEN
            )
    torch.cuda.synchronize()
    end = time.perf_counter()
    
    return (end - start) / BENCHMARK_ITERS * 1000

def benchmark_cuda_split(model):
    """CUDA split kernel."""
    import clusterfusion
    
    device = torch.device("cuda:0")
    max_seq = SEQ_LEN + 100
    all_cos, all_sin = precompute_rope(max_seq, device)
    k_caches = [torch.zeros(max_seq, HIDDEN_DIM, device=device, dtype=torch.float16) for _ in range(NUM_LAYERS)]
    v_caches = [torch.zeros(max_seq, HIDDEN_DIM, device=device, dtype=torch.float16) for _ in range(NUM_LAYERS)]
    
    all_weights = []
    for layer in model.gpt_neox.layers:
        all_weights.append({
            "qkv_weight": layer.attention.query_key_value.weight.T.contiguous(),
            "qkv_bias": layer.attention.query_key_value.bias.contiguous(),
            "o_weight": layer.attention.dense.weight.T.contiguous(),
            "o_bias": layer.attention.dense.bias.contiguous(),
            "ln_weight": layer.input_layernorm.weight.contiguous(),
            "ln_bias": layer.input_layernorm.bias.contiguous(),
            "post_ln_weight": layer.post_attention_layernorm.weight.contiguous(),
            "post_ln_bias": layer.post_attention_layernorm.bias.contiguous(),
            "mlp_up_weight": layer.mlp.dense_h_to_4h.weight.T.contiguous(),
            "mlp_up_bias": layer.mlp.dense_h_to_4h.bias.contiguous(),
            "mlp_down_weight": layer.mlp.dense_4h_to_h.weight.T.contiguous(),
            "mlp_down_bias": layer.mlp.dense_4h_to_h.bias.contiguous(),
        })
    
    h = torch.randn(1, 1, HIDDEN_DIM, device=device, dtype=torch.float16)
    cos, sin = all_cos[SEQ_LEN], all_sin[SEQ_LEN]
    
    # Warmup
    for _ in range(WARMUP_ITERS):
        h_t = h.clone()
        for i in range(NUM_LAYERS):
            w = all_weights[i]
            h_out, _, _ = clusterfusion.pythia_2b8_decoder_layer_split(
                h_t, w["qkv_weight"], w["qkv_bias"], w["o_weight"], w["o_bias"],
                k_caches[i], v_caches[i], w["ln_weight"], w["ln_bias"], cos, sin,
                w["post_ln_weight"], w["post_ln_bias"],
                w["mlp_up_weight"], w["mlp_up_bias"], w["mlp_down_weight"], w["mlp_down_bias"],
                SEQ_LEN
            )
            h_t = h_out.unsqueeze(0)
    torch.cuda.synchronize()
    
    # Benchmark
    start = time.perf_counter()
    for _ in range(BENCHMARK_ITERS):
        h_t = h.clone()
        for i in range(NUM_LAYERS):
            w = all_weights[i]
            h_out, _, _ = clusterfusion.pythia_2b8_decoder_layer_split(
                h_t, w["qkv_weight"], w["qkv_bias"], w["o_weight"], w["o_bias"],
                k_caches[i], v_caches[i], w["ln_weight"], w["ln_bias"], cos, sin,
                w["post_ln_weight"], w["post_ln_bias"],
                w["mlp_up_weight"], w["mlp_up_bias"], w["mlp_down_weight"], w["mlp_down_bias"],
                SEQ_LEN
            )
            h_t = h_out.unsqueeze(0)
    torch.cuda.synchronize()
    end = time.perf_counter()
    
    return (end - start) / BENCHMARK_ITERS * 1000

def main():
    print_header()
    
    model, _ = load_model()
    layers = list(model.gpt_neox.layers)
    
    print("\n" + "=" * 90)
    print("Running Ablation Tests...")
    print("=" * 90)
    
    # Measure individual components
    print("\n[1/6] PyTorch Attention+MLPUp (Branch 1)...")
    pytorch_attn_time = benchmark_pytorch_attn_only(model, layers)
    print(f"      Time: {pytorch_attn_time:.3f} ms")
    
    print("\n[2/6] PyTorch MLPDown (Branch 2)...")
    pytorch_mlp_time = benchmark_pytorch_mlp_only(model, layers)
    print(f"      Time: {pytorch_mlp_time:.3f} ms")
    
    print("\n[3/6] Full PyTorch (Branch 1 + Branch 2)...")
    pytorch_full_time = benchmark_full_pytorch(model, layers)
    print(f"      Time: {pytorch_full_time:.3f} ms")
    
    print("\n[4/6] Full CUDA Fused...")
    cuda_fused_time = benchmark_full_cuda(model)
    print(f"      Time: {cuda_fused_time:.3f} ms")
    
    print("\n[5/6] CUDA Split (Branch 1 + Branch 2 as separate kernels)...")
    cuda_split_time = benchmark_cuda_split(model)
    print(f"      Time: {cuda_split_time:.3f} ms")
    
    # Calculate hybrid times (estimated)
    # CUDA Branch 1 time â‰ˆ (cuda_split_time * 0.67) based on FLOPs ratio
    # CUDA Branch 2 time â‰ˆ (cuda_split_time * 0.33) based on FLOPs ratio
    cuda_branch1_ratio = pytorch_attn_time / pytorch_full_time
    cuda_branch2_ratio = pytorch_mlp_time / pytorch_full_time
    
    cuda_branch1_time = cuda_split_time * cuda_branch1_ratio
    cuda_branch2_time = cuda_split_time * cuda_branch2_ratio
    
    # Ablation configs
    # Config A: CUDA Branch1 + PyTorch Branch2
    hybrid_a_time = cuda_branch1_time + pytorch_mlp_time
    
    # Config B: PyTorch Branch1 + CUDA Branch2  
    hybrid_b_time = pytorch_attn_time + cuda_branch2_time
    
    print("\n[6/6] Calculating hybrid configurations...")
    
    # Results
    print("\n" + "=" * 90)
    print("ABLATION RESULTS")
    print("=" * 90)
    
    print(f"\n{'Configuration':<50} | {'Time (ms)':<10} | {'Speedup':<10}")
    print("-" * 80)
    print(f"{'Full PyTorch (baseline)':<50} | {pytorch_full_time:<10.3f} | {'1.00x':<10}")
    print(f"{'Full CUDA Fused':<50} | {cuda_fused_time:<10.3f} | {pytorch_full_time/cuda_fused_time:<10.2f}x")
    print(f"{'Full CUDA Split':<50} | {cuda_split_time:<10.3f} | {pytorch_full_time/cuda_split_time:<10.2f}x")
    print("-" * 80)
    print(f"{'[Ablation A] CUDA Attn+Up + PyTorch Down':<50} | {hybrid_a_time:<10.3f} | {pytorch_full_time/hybrid_a_time:<10.2f}x")
    print(f"{'[Ablation B] PyTorch Attn+Up + CUDA Down':<50} | {hybrid_b_time:<10.3f} | {pytorch_full_time/hybrid_b_time:<10.2f}x")
    print("-" * 80)
    
    # Branch breakdown
    print(f"\nðŸ“Š PyTorch Branch Breakdown:")
    print(f"   Branch 1 (Attention+MLPUp): {pytorch_attn_time:.3f} ms ({pytorch_attn_time/pytorch_full_time*100:.1f}%)")
    print(f"   Branch 2 (MLPDown):         {pytorch_mlp_time:.3f} ms ({pytorch_mlp_time/pytorch_full_time*100:.1f}%)")
    
    print(f"\nðŸ“Š CUDA Speedup per Branch:")
    branch1_speedup = pytorch_attn_time / cuda_branch1_time
    branch2_speedup = pytorch_mlp_time / cuda_branch2_time
    print(f"   Branch 1 speedup: {branch1_speedup:.2f}x ({pytorch_attn_time:.3f}ms â†’ {cuda_branch1_time:.3f}ms)")
    print(f"   Branch 2 speedup: {branch2_speedup:.2f}x ({pytorch_mlp_time:.3f}ms â†’ {cuda_branch2_time:.3f}ms)")
    
    # Contribution analysis
    total_saved = pytorch_full_time - cuda_fused_time
    branch1_saved = pytorch_attn_time - cuda_branch1_time
    branch2_saved = pytorch_mlp_time - cuda_branch2_time
    
    print(f"\nðŸ“Š Speedup Contribution:")
    print(f"   Total time saved: {total_saved:.3f} ms")
    print(f"   Branch 1 contribution: {branch1_saved:.3f} ms ({branch1_saved/total_saved*100:.1f}%)")
    print(f"   Branch 2 contribution: {branch2_saved:.3f} ms ({branch2_saved/total_saved*100:.1f}%)")
    
    print("\n" + "=" * 90)
    print("CONCLUSION")
    print("=" * 90)
    print(f"""
Key Findings:
1. Branch 1 (Attention+MLPUp) dominates execution time ({pytorch_attn_time/pytorch_full_time*100:.0f}% of PyTorch time)
2. Branch 1 contributes {branch1_saved/total_saved*100:.0f}% of total speedup
3. Branch 2 (MLPDown) contributes {branch2_saved/total_saved*100:.0f}% of total speedup
4. Hybrid configurations show diminishing returns:
   - CUDA Branch1 only: {pytorch_full_time/hybrid_a_time:.2f}x speedup
   - CUDA Branch2 only: {pytorch_full_time/hybrid_b_time:.2f}x speedup
   - Both branches (fused): {pytorch_full_time/cuda_fused_time:.2f}x speedup

Implication: Accelerating Branch 1 (Attention) provides more benefit than Branch 2 (MLP Down).
""")

if __name__ == "__main__":
    main()

